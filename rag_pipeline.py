import numpy as np
from embedding_utils import embed_query
from faiss_utils import load_faiss_index, load_chunks
from langchain_google_genai import ChatGoogleGenerativeAI
import os
from dotenv import load_dotenv

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

def search_and_retrieve(question: str, top_k: int = 3) -> list[str]:
    """
    Searches the FAISS index using the embedded question and retrieves the top_k most relevant text chunks.

    Args:
        question (str): The user's query to search within the indexed document.
        top_k (int): Number of most relevant chunks to retrieve from the FAISS index. Default is 3.

    Returns:
        List[str]: A list of the top_k retrieved context chunks most relevant to the query.
    """

    # Load the prebuilt FAISS index from disk
    faiss_index = load_faiss_index()

    # Load the original text chunks that were indexed
    context_chunks = load_chunks()

    # Convert the question into an embedding vector
    question_embedding = embed_query(str(question))

    # Perform a similarity search in the FAISS index
    # D contains distances, I contains indices of top_k nearest chunks
    distances, indices = faiss_index.search(question_embedding.astype(np.float32), top_k)

    # Retrieve the actual chunks using the matched indices
    retrieved_chunks = [context_chunks[i] for i in indices[0]]

    return retrieved_chunks

# def search_and_retrieve(question, top_k=3):
#     index = load_faiss_index()
#     chunks = load_chunks()
#     query_embedding = embed_query(question)
#     D, I = index.search(query_embedding.astype(np.float32), top_k)
#     retrieved_chunks = [chunks[i] for i in I[0]]
#     return retrieved_chunks

# def ask_gemini(question, retrieved_chunks):
#     context = "\n".join(retrieved_chunks)
#     prompt = f"Answer the question based on context:\n{context}\n\nQuestion: {question}"

#     llm = ChatGoogleGenerativeAI(
#         model="gemini-1.5-flash",
#         google_api_key=GOOGLE_API_KEY,
#         temperature=0.2
#     )

#     response = llm.invoke(prompt)
#     return response.content
def ask_gemini(question: str, retrieved_chunks: list[str]) -> str:
    """
    Generates an answer to a question using Google's Gemini language model,
    based on the provided context chunks.

    Args:
        question (str): The user query to be answered.
        retrieved_chunks (List[str]): List of text chunks retrieved from the knowledge base.

    Returns:
        str: The answer generated by the Gemini model.
    """

    # Combine the retrieved text chunks into a single string to use as context
    context_text: str = "\n".join([str(chunk) for chunk in retrieved_chunks])

    # Create a prompt that instructs the model to use the provided context to answer the question
    formatted_prompt: str = (
        f"Answer the question based on context:\n{context_text}\n\nQuestion: {str(question)}"
    )

    # Initialize the Gemini language model with desired configuration
    gemini_llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        google_api_key=GOOGLE_API_KEY,
        temperature=0.2  # Low temperature for more deterministic output
    )

    # Invoke the model with the constructed prompt and get the response
    response = gemini_llm.invoke(formatted_prompt)

    # Return the content of the model's response
    return str(response.content)
